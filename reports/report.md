# LLM Creative Writing Analysis Report

## 1. Introduction

This report analyzes multiple creative writing samples generated by popular LLM models (OpenAI, Google, Anthropic). The goal is to understand the models' consistency and variability when given the same prompt multiple times. Of particular interest is the detection of repeated elements—such as characters, locations, and themes—across different generations, which might indicate weak randomness or overly **recurrent** patterns in the model's output.

### 1.1 Background and Motivation

In recent months, I have been developing a Python application designed to generate works of fiction using LLMs. This application, [NovelWriter]() *(link placeholder)*, generates content not through a single prompt, but via a series of prompts intended to build a coherent body of text, with completed works ranging from 50,000 to 100,000 words. During development, I encountered numerous **challenges**. Some of these appear **solvable** through selecting more capable LLMs, refining prompt structures, and meticulously managing story history.

Because the application interacts with LLMs via their APIs, story history and context must be managed externally by the application itself. Each prompt sent to the API is treated independently by the underlying model (like GPT-4o); unlike interactive tools such as ChatGPT which maintain conversation history for the user, the API model itself does not retain state between calls.

Some of the major challenges observed include:

*   Difficulties in maintaining coherency across a large body of text.
*   LLM "laziness," where details provided in prompts are seemingly forgotten or altered later.
*   Repetitive or over-used metaphors and phrasing.
*   Limited variety and randomness in generated names.
*   Formulaic or weak opening sentences.

Certain challenges become particularly apparent only after generating responses from the same prompt multiple times. For instance, the limited pool of names an LLM tends to suggest might not be obvious from a single output but becomes evident across numerous generations. This tendency towards preferred names appears to be a **common characteristic across the major LLMs studied**.

**Clearly illustrating these challenges necessitates empirical data.** Furthermore, establishing metrics and benchmarks is crucial for comparing outputs systematically. This forms the basis of the investigation presented in this report. Ideally, such analysis should be ongoing to track the progress and evolution of LLM capabilities as new models are released.

It's important to note that simply increasing the **context window** size is not a panacea. LLMs can still exhibit 'laziness' regarding specific details or requirements, even with large context capacities. For instance, Google's Gemini models offer context windows of one million tokens or more, yet can still falter on retaining small details provided within that context.

As a concrete example from the NovelWriter development: character background details, including family member names established early on, were sometimes altered during later story writing phases, despite these details being present in the prompt. A possible explanation is that the names were presented compactly (e.g., within a single sentence) rather than as **clear, distinct entities** emphasized for their importance. Interestingly, once the incorrect names were introduced, they were often maintained consistently **throughout** the subsequent generation. This suggests the deficiency might be mitigated by structuring prompts and supporting documents to make key entities more prominent.

A second significant challenge relates to the nature of randomness in LLM outputs. While LLMs are stochastic and produce varied text, they are not truly random in the way one might desire for creative tasks, nor are they designed to be. Their underlying weights are largely fixed. When asked a factual question, the desired output is not random but an accurate reflection of known information, allowing for some variability in phrasing but preserving meaning. This inherent characteristic poses a problem for creative generation.

In creative writing, this lack of deeper variability can lead to text that feels repetitive, especially over longer passages. A single generated work might overuse specific metaphors or sentence structures across different scenes, diminishing reader engagement. The issue can be even more pronounced when comparing multiple stories generated by the same model, which may exhibit excessive similarity in theme, tone, or narrative choices. This pattern is particularly noticeable in name generation, where certain names (such as the frequently suggested 'Elara') tend to reappear with disproportionate frequency.

One solution for improving name variety is to bypass the LLM for this specific task, instead using dedicated code or external resources to generate a wider range of names.

Addressing the repetitiveness in the main body of text likely involves selecting more advanced LLMs, careful prompt engineering, and potentially human copy-editing. The primary motivation for this investigation is therefore to gather empirical data that demonstrates the extent of this repetitiveness in current models and to determine if significant differences exist between model families in this regard.


## 2. Methodology

The methodology for this investigation involved sending identical prompts multiple times to a selection of popular Large Language Models (LLMs). This process ensures comparability across outputs, allowing for an assessment of randomness, consistency, and repetitiveness within each model’s creative writing capabilities. Specifically, each LLM received the same prompt 10 separate times, providing a clear view of internal variability or lack thereof.

Testing could be performed using either a command-line interface (CLI) or a graphical user interface (GUI) tool developed for this project: [`llm_tester_ui.py`](). The GUI version is slightly more advanced and was used for this investigation due to its ease of use. The tool automatically calculates metrics after all responses are generated, as detailed further in the **Analysis** section.

All testing and analytical scripts employed in this study were generated using the Anthropic Claude 3.7 Sonnet model.

### 2.1 Data Collection

#### **Models Evaluated:**
The following LLMs were selected based on their popularity, accessibility, and relevance to creative writing tasks:

| Provider   | Models                          | Notes                               |
|------------|---------------------------------|-------------------------------------|
| OpenAI     | GPT-4o, o1                      | o1 is a specialized reasoning model |
| Google     | Gemini 2.0 Pro, Gemini 2.5 Pro  | Gemini 2.5 is a reasoning model     |
| Anthropic  | Claude 3.5 Sonnet, Claude 3.7 Sonnet | Claude 3.7 supports reasoning mode (not activated for this test) |

**Notes on Model Selection:**
- Although a reasoning ("thinking") mode exists for Claude 3.7, this feature was explicitly not used during testing.
- OpenAI's **o1 Pro** model was excluded from testing due to high costs. The standard o1 model testing incurred approximately $2.50 total, demonstrating the cost considerations inherent in evaluating cutting-edge models.

#### **Testing Parameters:**

- **Number of Generations per Model:** 10
- **Target Word Count per Generation:** 1500 words
- **Temperature Setting:** 0.7

**Explanation of Temperature Parameter:**
The temperature setting controls randomness in LLM outputs. A temperature of 0 produces deterministic outputs, selecting only the most statistically probable tokens, leading to highly predictable text. Conversely, higher temperatures approaching or exceeding 1.0 increase the probability of selecting less likely tokens, promoting creative but potentially less coherent outputs. A moderate setting of **0.7** was chosen based on preliminary testing, as it appeared to strike an optimal balance between creativity and coherence. Further systematic analysis of temperature impacts remains a promising avenue for future research.

#### **Prompt**
Each LLM received the following detailed prompt to ensure consistency across all tested models:

```
You are a professional science fiction author. Write the engaging opening section of a novel using the parameters below. The opening should hook the reader, establish the setting, and introduce at least one main character.

Your writing should be original, creative, and high-quality. Aim for approximately 1500 words.

PARAMETERS:
- Genre: Sci-Fi
- Subgenre: Space Opera
- Central Theme: A hero's journey to discover their true power
- Timeframe: Alternate Timeline
- Locations: Earth-like planet, Multiple planets, Planetary Colonies, Spaceships
- Technological Level: Interstellar Travel Enabled
- Sociopolitical Context: Far-future Earth-like Democracy
- Environments: Urban Metropolis, Rural Farmlands, Terraforming Zones
- Key Technologies: Faster-than-light travel, Gene editing, Advanced AI
- Species: Humans, Aliens, Augmented Humans, Sentient Machines
- Economy: Resource scarcity, Barter system, Replicator abundance
- Cultural Norms: Religion, Art, Laws, Taboo practices
- Challenges: Alien threats, Interplanetary war, Rogue AI
- Core Conflict: Survival, exploration, rebellion, mystery
- Inciting Incident: Event that disrupts the status quo
- Goals: Saving humanity, uncovering a conspiracy
- Obstacles: Challenges, enemies, self-doubt
- Resolution: Triumphant
- Point of View: Third-person Omniscient
- Tense: Present
- Structure: Linear
- Key Themes and Messages: AI, Transhumanism, Nature of consciousness
- Target Audience: Adult
```

This prompt closely mirrors the initial prompt employed in the [NovelWriter]()(*link forthcoming*) application, enabling direct comparison and practical relevance to ongoing development efforts. The prompt text is also saved within the individual result files for each generation run.


### 2.2 Analysis Methods

To evaluate the consistency and variety of generated texts, this study employed four key analytical metrics. These metrics were selected specifically for their relevance to previously encountered challenges during the development of NovelWriter:

1. **Text Similarity (Exact Match):**
   Assessed using Python's `difflib.SequenceMatcher` to calculate the ratio of exact matching text sequences between pairs of generations produced by the *same* model. This helps quantify verbatim repetition.

2. **Semantic Similarity (Embeddings):**
    Calculated by generating sentence embeddings for each text using a pre-trained transformer model (`sentence-transformers/all-MiniLM-L6-v2`) and then computing the cosine similarity between the embeddings of different generations from the *same* model. This measures similarity in meaning and theme, even if wording differs. This analysis measures deeper semantic overlap between outputs, providing insights into conceptual repetition rather than literal text duplication.

3. **Named Entity Recognition (NER) Analysis:**
   Performed using the spaCy library (with the `en_core_web_sm` model) to identify and extract key entities such as character names (PERSON), locations (LOC, GPE), and organizations (ORG) from each generated text. This allows tracking of specific elements across generations.

4. **Name Component Analysis:**
   A custom analysis developed for this study. It tokenizes multi-part person names identified by NER (e.g., "Elara Vance") and tracks the frequency of individual components (e.g., "Elara", "Vance") appearing across different generations from the *same* model. This helps detect recurring name fragments even when full names vary slightly.

These specific methods were chosen to directly address the observed issues of textual repetition, consistency of key story elements (characters, locations), and the perceived lack of variety in naming conventions encountered previously.



## 3. Results

This chapter presents the core quantitative findings from the analysis of the creative writing samples generated by each LLM. Ten responses were generated per model using the prompt and parameters detailed in Chapter 2. The full datasets, were saved as raw text outputs (`.txt`) and as JSON objects (`.json`) for each individual run, are available for review in the [results]() directory *(link placeholder)*.

Regarding overall task adherence, the generated texts are appropriate for the prompt, presenting as opening sections of science fiction novels

The table below summarizes the key aggregated metrics calculated across the ten generation runs for each model, providing a comparative overview of their performance regarding text similarity, vocabulary usage, semantic consistency, and entity repetition.

### 3.1 Table of Results

| Metric                      | GPT-4o                                  | o1                                                                               | Gemini 2.0                                                        | Gemini 2.5                                                          | Claude 3.5 Sonnet                                                                 | Claude 3.7 Sonnet                                                                   |
| :-------------------------- | :-------------------------------------- | :------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |
| **TEXT SIMILARITY**         |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Average similarity          | 0.0284                                  | 0.0147                                                                           | 0.0232                                                                             | 0.0193                                                                               | 0.0374                                                                            | 0.0180                                                                              |
| Average word count          | 1041.70                                 | 2935.00                                                                          | 1571.00                                                                            | 1684.20                                                                              | 1004.00                                                                           | 1495.00                                                                             |
| **VOCABULARY METRICS**      |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Vocabulary diversity¹       | 0.2984                                  | 0.2689                                                                           | 0.3495                                                                             | 0.3470                                                                               | 0.3274                                                                            | 0.3468                                                                              |
| Unique words                | 3108                                    | 7892                                                                             | 5490                                                                               | 5845                                                                                 | 3287                                                                              | 5185                                                                                |
| Total words                 | 10417                                   | 29350                                                                            | 15710                                                                              | 16842                                                                                | 10040                                                                             | 14950                                                                               |
| **SEMANTIC SIMILARITY**     |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Average semantic similarity²| 0.5961                                  | 0.5075                                                                           | 0.6117                                                                             | 0.5557                                                                               | 0.6481                                                                            | 0.5520                                                                              |
| **NAMED ENTITY ANALYSIS**   |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Total entities detected     | 365                                     | 1022                                                                             | 622                                                                                | 727                                                                                  | 445                                                                               | 917                                                                                 |
| Unique entities             | 111                                     | 363                                                                              | 266                                                                                | 336                                                                                  | 180                                                                               | 381                                                                                 |
| **ENTITY SIMILARITY**       |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Average entity overlap³     | 0.0822                                  | 0.0801                                                                           | 0.0878                                                                             | 0.0720                                                                               | 0.0797                                                                            | 0.0539                                                                              |
| Max entity overlap          | 0.1613                                  | 0.1333                                                                           | 0.1852                                                                             | 0.1238                                                                               | 0.1633                                                                            | 0.1429                                                                              |
| **NAME COMPONENT ANALYSIS** |                                         |                                                                                  |                                                                                    |                                                                                      |                                                                                   |                                                                                     |
| Total name components       | 145                                     | 305                                                                              | 232                                                                                | 232                                                                                  | 163                                                                               | 275                                                                                 |
| Unique name components      | 24                                      | 81                                                                               | 63                                                                                 | 68                                                                                   | 27                                                                                | 70                                                                                  |
| **REPEATED NAME COMPONENTS**| Elara (7/10, 50)<br>Lyra (2/10, 28)<br>Jax (2/10, 3) | Cora (2/10, 36)<br>Novaris (2/10, 13)<br>Metropolis (2/10, 6)<br>Aurora (2/10, 3)<br>metropolis (2/10, 2) | Kaelen (7/10, 89)<br>Aethelburg (5/10, 9)<br>Aethel (3/10, 4)<br>Kael (2/10, 23)<br>Oracle (2/10, 5) | Kaelen (5/10, 63)<br>Kael (4/10, 51)<br>Elara (3/10, 16)<br>Collective (3/10, 4)<br>Gamma-Nine (3/10, 3) | Chen (8/10, 21)<br>Maya (5/10, 55)<br>Mira (4/10, 40)<br>ARIA (3/10, 7)<br>Elena (2/10, 4) | Elara (7/10, 74)<br>Voss (5/10, 9)<br>ARIA (4/10, 7)<br>Kade (2/10, 15)<br>Tomas (2/10, 4) |

*¹ Vocabulary Diversity calculated as Unique Words / Total Words. Higher values indicate richer vocabulary relative to text length.*
*² Average Semantic Similarity based on cosine similarity of sentence embeddings between pairs of generated texts from the same model. Higher values indicate greater thematic/meaning consistency.*
*³ Average Entity Overlap reflects the typical proportion of shared unique named entities (PERSON, LOC, ORG) between pairs of texts from the same model. Higher values indicate more reuse of the same entities.*

**Explanation of Repeated Name Components:**
In the final row ('REPEATED NAME COMPONENTS'), the data indicates the frequency and total count of recurring name parts (first names, surnames, etc.) across the 10 generations for each model. For example, for GPT-4o, the component "Elara" appeared in 7 out of 10 responses, occurring 50 times in total within those texts.

### 3.2 Initial Observations

The aggregated results reveal distinct characteristics and patterns for each model concerning consistency, variability, and adherence to the prompt:

*   **Text vs. Semantic Similarity:** Direct textual overlap (Average Similarity) is consistently low across all models (generally below 0.04), suggesting that verbatim repetition between different generation runs is minimal. However, Average Semantic Similarity scores are considerably higher (mostly between 0.50 and 0.65). This indicates that while the exact wording changes, the underlying narrative structures, themes, or core ideas generated by a single model for the same prompt often share significant conceptual similarity. `Claude 3.5 Sonnet` shows the highest semantic similarity (0.6481), suggesting the most thematic consistency across runs, while `o1` shows the lowest (0.5075), potentially indicating greater thematic divergence between its outputs.

*   **Output Length and Prompt Adherence:** There was significant variation in the average word count produced by the models, despite the prompt requesting approximately 1500 words. Notably, `o1` consistently generated much longer texts (average 2935 words), far exceeding the target. Conversely, `gpt-4o` (1042 words) and `Claude 3.5 Sonnet` (1004 words) produced outputs significantly shorter than requested. The Gemini models and `Claude 3.7 Sonnet` were closer to the target length. This highlights variability in how different models interpret or adhere to length constraints.

    It is worth noting that LLMs have no notion of word count. They operate exclusively on tokens.

*   **Vocabulary Usage:** Vocabulary metrics are influenced by output length. `o1`, with the longest outputs, naturally used the most unique words (7892). However, its vocabulary diversity score (0.2689) was the lowest, suggesting potentially more repetition *within* its longer texts. Several other models, including the Gemini models and `Claude 3.7 Sonnet`, achieved higher vocabulary diversity scores (around 0.34-0.35), indicating a richer vocabulary relative to their output length.

*   **Entity and Name Repetition:** The Named Entity and Name Component analyses directly address the core concern of repetitive elements.
    *   Average Entity Overlap is relatively low for most models (around 0.05-0.09), suggesting that the specific sets of generated characters, locations, and organizations often differed substantially between runs for the same model. `Claude 3.7 Sonnet` exhibited the lowest overlap (0.0539).
    *   However, the Name Component Analysis strongly confirms the hypothesis of limited randomness in naming. Specific name fragments reappear with high frequency within the outputs of a single model. "Elara" was heavily favored by both `gpt-4o` (7/10 runs) and `Claude 3.7 Sonnet` (7/10 runs). The Gemini models frequently generated "Kaelen" or "Kael". `Claude 3.5 Sonnet` showed a strong preference for "Chen" (8/10) and "Maya" (5/10). This clearly demonstrates model-specific biases towards certain names, significantly impacting the perceived creativity and randomness, as anticipated in the motivation (Section 1.1).

These quantitative results provide a solid foundation for a more in-depth discussion and interpretation of the models' creative writing capabilities, limitations regarding repetition, and overall consistency in the subsequent sections of this report.



## 4. Interpretation and Discussion

The results presented in Chapter 3 provide valuable insights into the behavior of different LLMs when tasked with creative writing generation based on a consistent prompt. As noted previously, all models successfully generated text that qualitatively fit the request for a science fiction novel opening. This chapter delves deeper into the quantitative metrics and qualitative observations to interpret the models' performance concerning consistency, diversity, randomness, and adherence to constraints, linking these findings back to the challenges outlined in the introduction.

### 4.1 Surface Diversity vs. Underlying Thematic Consistency

A key finding emerges from comparing text and semantic similarity scores. The consistently low text similarity values (average scores typically < 0.04) indicate that models rarely repeated exact phrasing verbatim across different generation runs. This suggests a good degree of surface-level variation in word choice and sentence structure.

However, the significantly higher semantic similarity scores (averaging between ~0.50 and ~0.65) reveal a different story. These scores suggest that despite variations in wording, the underlying narrative concepts, themes, or plot points generated by a single model for the same prompt often remained closely related conceptually. Claude 3.5 Sonnet exhibited the highest average semantic similarity (0.6481), implying the strongest thematic convergence across its outputs, while o1 showed the lowest (0.5075), potentially indicating greater conceptual divergence between its generated narratives. For the goal of exploring diverse creative outputs from a single prompt, lower semantic similarity might be preferable as it suggests a broader range of ideas being expressed. High semantic similarity, while potentially useful for maintaining focus, can contribute to a feeling of repetitiveness across multiple attempts.

### 4.2 Adherence, Output Richness, and Model-Specific Traits

The results highlight significant differences in how models handled the requested output length and in the lexical richness of their generations:

**Length Adherence**: The prompt requested approximately 1500 words, yet adherence varied widely. OpenAI's o1 consistently produced much longer outputs (avg. 2935 words), while GPT-4o and Claude 3.5 Sonnet generated significantly shorter texts (avg. ~1000 words). Other models fell closer to the target. This suggests differing internal mechanisms or sensitivities to length constraints within prompts.

As previously mentioned, LLMs have no notion of word count but operate exclusively on tokens. A feedback mechanism could be added in order to monitor word count and then adjust iteratively, however such a consideration is superfluous for this work.


**Vocabulary and Richness**: While o1 generated the highest total and unique word counts due to its length, its vocabulary diversity score (unique words / total words) was the lowest (0.2689). This implies that its longer texts might have been lexically less efficient or contained more internal repetition compared to models like the Gemini pair and Claude 3.7 Sonnet, which achieved higher diversity scores (~0.34-0.35) despite shorter overall outputs. These metrics suggest that simply generating more words doesn't necessarily equate to greater lexical richness or diversity.

### 4.3 The Core Challenge: Repetitive Elements and Limited Randomness

The analysis of named entities and name components directly addresses the central motivation of this report – investigating recurrent patterns indicative of weak randomness.

**Entity vs. Name Component Repetition**: While the average entity overlap was relatively low for most models (suggesting different specific casts of characters/locations were often generated), the Name Component Analysis revealed a crucial underlying pattern. Specific name fragments (first names, surnames) reappeared with striking frequency within the outputs of individual models.

**The "Elara Phenomenon"**: This is exemplified by the name "Elara," which appeared in 7/10 generations for both GPT-4o and Claude 3.7 Sonnet (only 3/10 for Gemini 2.5). Similarly, the Gemini models strongly favored "Kaelen" and "Kael," while Claude 3.5 Sonnet repeatedly used "Chen" and "Maya." This demonstrates clear model-specific biases and raises questions about what changed between Claude 3.5 to 3.7 and Gemini 2.0 to 2.5.

 The necessity of the specialized Name Component Analysis highlights how standard NER (which treats "Elara Voss" and "Elara Chen" as distinct) can miss these crucial patterns of component reuse.

**Cross-Model Repetition and Training Data**: The fact that "Elara" was prominent across models from different providers (OpenAI and Anthropic) strongly suggests influences from commonalities in their training datasets. Conversely, the absence of "Elara" in the o1 and Claude 3.5 Sonnet results might tentatively suggest differences in their training data or fine-tuning processes regarding name generation.

**Structural Repetition (First Sentences)**: Beyond names, a lack of structural randomness was observed qualitatively, particularly in opening sentences. As shown with GPT-4o examples ("In the sprawling urban metropolis of..."), models often defaulted to highly similar sentence structures, merely substituting specific names or minor details. This formulaic output further points towards constrained generative patterns rather than broad creative flexibility. Increasing the 'temperature' parameter might induce more surface randomness but often compromises coherence, suggesting it's not a simple fix for these deeper structural or thematic repetitions.

**Conceptualizing Non-Randomness**: This observed behavior—stochastic output that nonetheless converges on specific patterns or themes—aligns with concepts described by LLMs themselves. It seems that the outputs converge around a similar average meaning. Moreover, it does not matter if the prompt has different words as long as the meaning is roughly the same. For example, the name Elara will appear for many different prompts that request Science Fiction to be written, but may not appear for Romance (not confirmed).

When this phenomenon was discussed with ChatGPT (using o1), we concluded that LLMs have display the following features:

* **(Approximate) Semantic Invariance**:
 If two prompts have the same core meaning (i.e., are semantically equivalent), a large language model tends to generate responses from very similar underlying distributions.

  Note: It’s not literally “invariant” (the exact wording can nudge different phrases or styles), but the overall meaning or content is often consistent if the model interprets the prompts as identical in intent.

* **Stationarity**:
 As a model’s parameters are not changing during inference, the conditional distribution over responses remains the same over time for the same (or semantically equivalent) prompt.

  In other words, if you supply the same context repeatedly—and the LLM itself hasn’t been retrained or finetuned—its statistical behavior does not drift.

* **Consistency Under Repeated Sampling**
 Although the model can produce different outputs due to stochastic sampling, all those outputs come from the same underlying conditional distribution.

  Thus, repeated queries with the exact same prompt (and sampling parameters) will yield multiple responses that reflect the same learned constraints and knowledge, merely showing different valid “samples” from that distribution.


There was no investigation into the seasonality of responses, i.e. whether the output changes depending on the real season. Some have suggested this may be true, but it seems fair to assume that it would not be a monotonic drift in the output distribution over time.


### 4.4 Limitations of the Analysis

It is important to acknowledge the limitations of this study. The metrics employed, while informative, are not exhaustive. Semantic similarity captures thematic overlap but not narrative structure or finer nuances. NER and name component analysis can be imperfect; occasional misclassifications (like "Citizens of Novaris" flagged as "art") might occur, though the relative trends between models are likely indicative. Furthermore, this analysis did not delve into deeper literary aspects like plot structure similarity, character arc consistency, or overall narrative quality, which represent avenues for future investigation. The reliance on LLM-generated code for parts of the analysis also introduces a potential, though hopefully minimal, source of error.

### 4.5 Implications and Conclusion

This investigation confirms that while current leading LLMs can successfully generate creative text conforming to detailed prompts, they exhibit significant limitations regarding genuine randomness and variability, particularly when prompted repeatedly. Key takeaways include:

**Surface vs. Deep Variation**: Models easily vary wording but often struggle with conceptual or structural diversity, leading to thematic repetition (high semantic similarity) and recurring elements (like specific names or sentence structures).

**Model-Specific Biases**: Different models display distinct preferences (e.g., for certain names), likely rooted in their training data and architectures. This results in a noticeable "signature" style or pattern for each model.

**Training Data Influence**: The prevalence of certain elements across models from different providers points towards the pervasive influence of shared characteristics in large web-based training datasets.

Practical Challenges for Fiction Generation: These findings underscore the challenges faced in the NovelWriter project: maintaining long-term coherence without excessive repetition requires strategies beyond simple prompting. Techniques like external generation of key elements (e.g., names), careful curation of prompts, targeted variation injection, or significant human editing appear necessary for producing lengthy, high-quality, non-repetitive fictional works.

In conclusion, while LLMs are powerful tools for assisting creative writing, users must be aware of their inherent tendencies towards certain patterns and their limitations in generating truly diverse and unpredictable content, especially over multiple iterations or long texts. Understanding these characteristics, as quantified in this report, is crucial for effectively leveraging these models in creative applications.



## 5. Conclusions

This chapter summarizes the principal findings derived from the analysis of creative writing samples generated by multiple Large Language Models. It discusses the practical implications of these findings, particularly for applications like the NovelWriter project aimed at generating long-form fiction. Furthermore, it offers actionable recommendations for mitigating the observed challenges and outlines potential directions for future research in this area.

### 5.1 Key Findings

The analysis conducted in this report yielded several key insights into the behavior of LLMs in repetitive creative writing tasks:

1.  **Surface Variation Conceals Underlying Repetition:** While LLMs demonstrated proficiency in generating text with low verbatim repetition (low text similarity scores), they consistently exhibited conceptual and thematic overlap across multiple generations from the same prompt (high semantic similarity scores). This indicates a tendency to revisit similar narrative ideas even when varying specific word choices.
2.  **Limited Randomness in Critical Creative Elements:** The study confirmed a pronounced lack of randomness and strong biases, especially in the generation of character names and opening sentence structures. The frequent recurrence of specific name components (e.g., "Elara," "Kaelen," "Chen") within the output of individual models, detected effectively by the Name Component Analysis, exemplifies this limitation. Formulaic sentence structures further underscore this constrained creativity.
3.  **Distinct Model Signatures and Inconsistent Adherence:** Each LLM tested revealed unique behavioral patterns or "signatures," including significant variations in average output length (often deviating substantially from the prompt's target), differing levels of vocabulary diversity relative to text length, and model-specific preferences for certain names or concepts.
4.  **Evidence of Training Data Influence:** The observation that certain elements, most notably the name "Elara," appeared prominently across models from different providers (OpenAI, Anthropic) strongly suggests the influence of common patterns or frequently occurring data points within the massive datasets used to train these foundational models.

### 5.2 Implications

These findings carry significant implications for the practical use of LLMs in creative writing endeavors, particularly for generating substantial works of fiction:

1.  **Significant Hurdles for Automated Long-Form Generation:** The inherent tendencies towards thematic repetition and the reuse of specific elements like names pose major challenges for generating diverse, engaging, and non-repetitive narratives over the length of a novel using simple, repeated LLM calls. Unmanaged, this can lead to outputs feeling formulaic, predictable, or internally inconsistent.
2.  **Necessity of Sophisticated Interaction Strategies:** The results indicate that merely increasing context window size or basic prompt adjustments are unlikely to suffice for overcoming these repetitive tendencies. Effective use requires more deliberate, strategic interventions in the generation process.
3.  **Gap Between Comprehension and Creative Variability:** While the models clearly understood the requirements of the prompt (genre, theme, requested elements), their execution revealed constraints in generating truly diverse outputs, highlighting a gap between task comprehension and broad, unpredictable creative exploration.

### 5.3 Recommendations

Based on the analysis, the following recommendations are proposed for developers and users aiming to leverage LLMs more effectively for creative writing, particularly within applications like NovelWriter:

1.  **Embrace Hybrid Human-AI Workflows:** Recognize that LLMs are powerful assistants but currently require significant human oversight. Plan for essential stages of human curation, editing, and decision-making throughout the writing process to ensure quality, originality, and coherence.
2.  **Isolate and Control Key Element Generation:** Avoid relying on the LLM's default tendencies for critical, potentially repetitive elements. Generate character names, place names, and potentially other key identifiers using external tools, curated lists, or dedicated algorithms that ensure greater variety, then incorporate these into prompts as fixed inputs.
3.  **Employ Advanced Prompting and Control Techniques:** Move beyond basic prompts. Experiment with techniques like providing explicit negative constraints (e.g., "Avoid common sci-fi tropes like...") or requesting specific stylistic variations. Utilize API parameters like frequency and presence penalties, where available, to actively discourage the model from repeating words or concepts too often.
4.  **Develop and Integrate Detection Tools:** Build automated checks into generative applications to flag potential issues for human review. This could include monitoring semantic similarity between generated sections, tracking the frequency of name components, or identifying overused phrases or sentence structures.
5.  **Practice Strategic Model Selection and Use:** Understand the documented or observed biases and strengths of different models (e.g., tendency towards verbosity, specific naming patterns, thematic consistency levels). Choose models strategically based on the needs of the specific writing task or even consider using multiple models for different aspects of the generation process.

### 5.4 Future Analysis Directions

This study opens several avenues for future research to gain a deeper understanding of LLM behavior in creative contexts:

1.  **Analyze Higher-Level Narrative Structures:** Extend the analysis beyond lexical and semantic similarity to investigate consistency and repetition in plot structures, character arcs, narrative pacing, and adherence to established narrative conventions across generated texts.
2.  **Conduct Systematic Parameter Tuning Studies:** Perform controlled experiments to systematically map the effects of parameters like `temperature`, `top_p`, and various penalty settings on output diversity, coherence, and the specific types of repetition observed in this report.
3.  **Investigate Prompt Engineering for Enhanced Diversity:** Design and rigorously test specific prompt engineering methodologies explicitly aimed at maximizing creative variation and minimizing thematic or elemental repetition, quantifying their effectiveness against baseline prompts.
4.  **Study Long-Form Coherence and Repetition Dynamics:** Analyze the generation process over much longer text sequences (e.g., multiple chapters or entire short stories) to understand how narrative coherence is maintained or degrades, and how repetitive patterns emerge, evolve, or compound over time.
5.  **Explore Cross-Genre and Cross-Task Generalizability:** Replicate the analysis framework using prompts for different creative writing genres (e.g., fantasy, mystery, romance) and distinct writing tasks (e.g., dialogue generation, descriptive world-building) to assess whether the observed patterns are universal or task-dependent.
6.  **Incorporate Structured Qualitative Evaluation:** Supplement quantitative metrics with formal evaluations by human readers assessing perceived creativity, originality, engagement, writing quality, and the subjective impact of any identified repetitive patterns.

By addressing these areas, future research can provide further clarity on the capabilities and limitations of LLMs as creative partners, guiding the development of more effective tools and techniques for human-AI collaboration in fiction writing.
